# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the BSD 3-Clause license found in the
# LICENSE file in the root directory of this source tree.

import types
from dataclasses import dataclass
from functools import partial
from typing import Union

import torch
import torch.nn as nn
from fbgemm_gpu.experimental.gen_ai.quantize import int4_row_quantize_zp, pack_int4

from torchao.core.config import AOBaseConfig
from torchao.quantization import Int4Tensor, Int8Tensor
from torchao.quantization.granularity import PerRow
from torchao.quantization.quant_api import (
    Int4WeightOnlyConfig,
    Int8WeightOnlyConfig,
    _module_extra_repr,
)
from torchao.quantization.quantize_.workflows.int4.int4_tensor import (
    _int4_row_dequantize_zp,
    _int4_row_quantize_zp_precomputed_qparams,
)
from torchao.quantization.transform_module import register_quantize_module_handler
from torchao.quantization.utils import get_block_size

from .observer import GPTQObserverTensor

CONFIG_TO_TORCHAO_BASE_TENSOR = {
    Int4WeightOnlyConfig: Int4Tensor,
    Int8WeightOnlyConfig: Int8Tensor,
}


@dataclass
class GPTQConfig(AOBaseConfig):
    """Config for GPTQ quantization

    step="observe": wraps weights as GPTQObserverTensor for observation.
    step="convert": applies GPTQ quantization to observed tensors.

    Args:
        step: Either "observe" or "convert"
        base_config: Base quantization configuration that determines the target dtype.
            Use Int4WeightOnlyConfig() for int4 or Int8WeightOnlyConfig() for int8.
        percdamp: Damping factor for Hessian
        gptq_quantize_block_size: Block size for GPTQ algorithm
    """

    step: str = "observe"  # "observe" or "convert"
    base_config: Union[Int4WeightOnlyConfig, Int8WeightOnlyConfig] = None
    percdamp: float = 0.01
    gptq_quantize_block_size: int = 256

    def __post_init__(self):
        if self.base_config is None:
            self.base_config = Int4WeightOnlyConfig(group_size=128)


@register_quantize_module_handler(GPTQConfig)
def _gptq_config_transform(
    module: torch.nn.Module, config: GPTQConfig, *, parameter_name="weight"
) -> torch.nn.Module:
    """Unified transform handler that uses explicit step control."""
    tensor = getattr(module, parameter_name)

    if config.step == "observe":
        # Observation phase: wrap as GPTQObserverTensor
        new_tensor = GPTQObserverTensor.from_hp(tensor)
        setattr(module, parameter_name, nn.Parameter(new_tensor, requires_grad=False))
        module.extra_repr = types.MethodType(
            partial(
                _module_extra_repr,
                original_extra_repr=module.extra_repr,
                parameter_name=parameter_name,
            ),
            module,
        )
        return module
    elif config.step == "convert":
        # Quantization phase: tensor should be an GPTQObserverTensor
        if not isinstance(tensor, GPTQObserverTensor):
            raise ValueError(
                f"Expected {parameter_name} to be GPTQObserverTensor in 'convert' step, "
                f"but got {type(tensor)}. Did you run the 'observe' step first?"
            )

        # Validate that observations were recorded
        if tensor.hessian is None:
            raise ValueError(
                f"No observations recorded for {parameter_name}. "
                f"Hessian is None. Did you run forward passes during the observe step?"
            )

        # Use pre-computed Hessian directly
        hessian = tensor.hessian
        new_tensor = gptq_quantize(hessian, tensor.hp_data, config)
        new_quantized_tensor = nn.Parameter(new_tensor, requires_grad=False)
        setattr(module, parameter_name, new_quantized_tensor)
        return module
    else:
        raise ValueError(
            f"Invalid step '{config.step}'. Must be 'observe' or 'convert'."
        )


def gptq_quantize(H: torch.Tensor, W: torch.Tensor, config: GPTQConfig):
    """
    This function implements the GPTQ algorithm described in this paper: https://arxiv.org/abs/2210.17323 (Algorithm 1)

    GPTQ quantizes weights column-by-column while propagating quantization errors to subsequent columns.
    This minimizes the overall error: argmin W' ||WX - W'X||_2^2

    For example we can seee how GPTQ improves accuracy on a toy example:(2x2 matrix, 2-bit symmetric quantization):

    Example:
        Given: W = [[1.2, 0.8],    X = [[1.0],      Original output: W @ X = [[2.0],
                    [0.5, 1.5]]         [1.0]]                                [2.0]]

        We can find the naive RTN quantization error as follows:

        1. Compute damped inverse Hessian: H_inv = [[0.505, -0.495], [-0.495, 0.505]]

        2. Quantize column 0: [1.2, 0.5]
           Scale: max(abs(w)) / quant_max = 1.2 / 2 = 0.6
           Quant: round(w / scale).clamp(0, 2) → [2, 1]
           Dequant: quant * scale → [1.2, 0.6]
           Error: err = ([1.2, 0.5] - [1.2, 0.6]) / 0.505 = [0.0, -0.198]
           Update column 1: W[:, 1] = [0.8, 1.5] - [0.0, -0.198] @ [-0.495] = [0.8, 1.402]

        3. Quantize column 1 (updated): [0.8, 1.402]
           Scale: 1.402 / 2 = 0.701
           Quant: [1, 2]
           Dequant: [0.701, 1.402]

        GPTQ result:  quantized [[2, 1], [1, 2]]  →  dequantized [[1.2,   0.701],
                                                                  [0.6,   1.402]]
                      W_gptq @ X = [[1.901], [2.002]]    Error: ||W@X - W_gptq@X||_2^2 = 0.09

        Compare this to naive RTN quantization:
        Naive (RTN): Column 0 scale: 1.2 / 2 = 0.6,  Column 1 scale: 1.5 / 2 = 0.75
                     quantized [[2, 1], [1, 2]]  →  dequantized [[1.2,  0.75],
                                                                 [0.6,  1.5 ]]
                     W_naive @ X = [[1.95], [2.1]]   Error: ||W@X - W_naive@X||_2^2 = 0.11

    Args:
        H: Hessian matrix approximation (from input activations)
        W: Weight matrix to quantize
        config: GPTQ configuration

    Returns:
        Int4Tensor or Int8Tensor: Quantized weight matrix
    """
    assert W.dim() == 2
    gptq_quantize_block_size = config.gptq_quantize_block_size
    percdamp = config.percdamp
    base_config = config.base_config

    if isinstance(base_config, Int4WeightOnlyConfig):
        group_size = config.base_config.group_size
        block_size = [1, group_size]
    elif isinstance(base_config, Int8WeightOnlyConfig):
        assert isinstance(base_config.granularity, PerRow), (
            "GPTQ only supports per-row quantization"
        )
        block_size = get_block_size(W.shape, base_config.granularity)
        block_size = list(block_size)
        group_size = block_size[-1]

    assert group_size > 0

    W = W.view(-1, W.shape[-1]).detach()
    columns = W.shape[1]
    device = W.device

    dead = torch.diag(H) == 0
    H[dead, dead] = 1
    W[:, dead] = 0

    # Apply damping and compute inverse Hessian for numerical stability
    damp = percdamp * torch.mean(torch.diag(H))
    diag = torch.arange(columns, device=device)
    H[diag, diag] += damp
    H = torch.linalg.cholesky(H)
    H = torch.cholesky_inverse(H)
    H = torch.linalg.cholesky(H, upper=True)
    Hinv = H

    group_qparams = []
    # If we are doing per-row quantization, the group_size is equal to the number of columns and this will only run once.
    # Otherwise, if we do per-group quantization, we need to iterate through the block one group at a time.
    for W_quantize_block, block_start in zip(
        torch.split(W, gptq_quantize_block_size, dim=1),
        range(0, columns, gptq_quantize_block_size),
    ):
        block_end = min(block_start + gptq_quantize_block_size, columns)
        Err1 = torch.zeros_like(W_quantize_block, dtype=H.dtype)
        Hinv_quantize_block = Hinv[block_start:block_end, block_start:block_end]

        for group_start in range(block_start, block_end, group_size):
            group_end = min(group_start + group_size, block_end)

            # We only need to calculate initial qparams for the group once
            if group_start % group_size == 0:
                if isinstance(base_config, Int4WeightOnlyConfig):
                    _, scale, zero_point = int4_row_quantize_zp(
                        W_quantize_block[
                            :, group_start - block_start : group_end - block_start
                        ],
                        group_size,
                    )
                    group_qparams.append((scale, zero_point))
                elif isinstance(base_config, Int8WeightOnlyConfig):
                    quantized_tensor = Int8Tensor.from_hp(
                        W_quantize_block[
                            :, group_start - block_start : group_end - block_start
                        ],
                        base_config.granularity,
                    )

            # Quantize each column and propagate errors to subsequent columns
            for i in range(group_start - block_start, group_end - block_start):
                w = W_quantize_block[:, i].unsqueeze(1)
                if isinstance(base_config, Int4WeightOnlyConfig):
                    q = _int4_row_quantize_zp_precomputed_qparams(
                        w, scale, zero_point, group_size
                    )
                    dq = _int4_row_dequantize_zp(q, scale, zero_point, group_size)
                elif isinstance(base_config, Int8WeightOnlyConfig):
                    q = Int8Tensor.from_hp(
                        w,
                        granularity=base_config.granularity,
                        scale=quantized_tensor.scale,
                    )
                    dq = q.dequantize(output_dtype=torch.float)

                err1 = (w - dq) / Hinv_quantize_block[i, i]
                W_quantize_block[:, i:] -= err1.matmul(
                    Hinv_quantize_block[i, i:].unsqueeze(0)
                )
                Err1[:, i] = err1.flatten()

        # Lazy Batch-Updates: We process B columns at a time with local updates above.
        # Once a block is fully processed, perform global updates to H^-1 and W using batched versions of the error propagation equations.
        W[:, block_end:] -= Err1.matmul(Hinv[block_start:block_end, block_end:])

    if "cuda" in device.type:
        torch.cuda.synchronize()

    # Create the final quantized tensor, which has the same qparams (scale, zero_point), but different qdata
    if isinstance(base_config, Int4WeightOnlyConfig):
        scale, zero_point = [torch.cat(x, dim=0) for x in zip(*group_qparams)]
        wq = _int4_row_quantize_zp_precomputed_qparams(W, scale, zero_point, group_size)
        result = Int4Tensor(
            qdata=pack_int4(wq),
            scale=scale.to(W.dtype),
            zero_point=zero_point.to(W.dtype),
            block_size=block_size,
            shape=W.shape,
            act_pre_scale=None,
        )
    elif isinstance(base_config, Int8WeightOnlyConfig):
        result = Int8Tensor.from_hp(
            W, granularity=base_config.granularity, scale=quantized_tensor.scale
        )

    return result


__all__ = [
    "GPTQConfig",
    "gptq_quantize",
]
